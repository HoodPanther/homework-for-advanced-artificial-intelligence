{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第11回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題. RNN Encoder-Decoderで日中翻訳のモデルを実装せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データのtrain_X, train_yのみが与えられます\n",
    "    - train_Xとtrain_yをtrain_X, valid_xとtrain_y, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - **test関数を戻り値**としてください (下に書いてあります)\n",
    "- **test_X, test_yに対する交差エントロピー(負の対数尤度)の平均で評価**します\n",
    "- 全体の実行時間がiLect上で60分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のような内容のコードが事前に実行されます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from __future__ import division\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "trng = RandomStreams(42)\n",
    "\n",
    "def build_vocab(file_path):\n",
    "    f_vocab, e_vocab = set(), set()\n",
    "    for line in open(file_path):\n",
    "        f, e = [l.strip().split()[1:-1] for l in line.split('|||')]\n",
    "        f_vocab.update(f)\n",
    "        e_vocab.update(e)\n",
    "    \n",
    "    f_w2i = {w: np.int32(i+2) for i, w in enumerate(f_vocab)}\n",
    "    e_w2i = {w: np.int32(i+2) for i, w in enumerate(e_vocab)}\n",
    "    \n",
    "    f_w2i['<s>'], f_w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "    e_w2i['<s>'], e_w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "    return set(f_w2i.keys()), set(e_w2i.keys()), f_w2i, e_w2i\n",
    "    \n",
    "def encode(sentence, vocab, w2i):\n",
    "    encoded_sentence = []\n",
    "    for w in sentence:\n",
    "        if w in vocab:\n",
    "            encoded_sentence.append(w2i[w])\n",
    "        else:\n",
    "            encoded_sentence.append(w2i['UNK'])\n",
    "    return encoded_sentence\n",
    "    \n",
    "def decode(encoded_sentence, w2i):\n",
    "    i2w = {i:w for w, i in w2i.items()}\n",
    "    decoded_sentence = []\n",
    "    for i in encoded_sentence:\n",
    "        decoded_sentence.append(i2w[i])\n",
    "    return decoded_sentence\n",
    "    \n",
    "def load_data(file_path, f_vocab, e_vocab, f_w2i, e_w2i):\n",
    "    x, y = [], []\n",
    "    for line in open(file_path):\n",
    "        f, e = [l.strip().split() for l in line.split('|||')]\n",
    "        f_enc = encode(f, f_vocab, f_w2i)\n",
    "        e_enc = encode(e, e_vocab, e_w2i)\n",
    "        x.append(f_enc)\n",
    "        y.append(e_enc)\n",
    "    return x, y\n",
    "\n",
    "global f_vocab\n",
    "global e_vocab\n",
    "\n",
    "f_vocab, e_vocab, f_w2i, e_w2i = build_vocab(dataset_path)\n",
    "train_X, train_y = load_data(dataset_path, f_vocab, e_vocab, f_w2i, e_w2i)\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.2, random_state=??) # random_stateはひみつです\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルのhomework関数を完成させて提出してください\n",
    "- **上記のコード以外で必要なもの**は全て書いてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ilect": {
     "is_homework": true
    }
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y):\n",
    "    import time\n",
    "    \n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # helper function for initialization\n",
    "    def sharedX(X, name=None, dtype=\"float32\"):\n",
    "        return theano.shared(np.array(X, dtype=dtype), name=name)\n",
    "    \n",
    "    # 1. word embedding\n",
    "    class Projection:\n",
    "        def __init__(self, in_dim, out_dim, scale):\n",
    "            self.V = sharedX(rng.randn(in_dim, out_dim) * scale, name='V')\n",
    "            self.params = [self.V]\n",
    "\n",
    "        def f_prop(self, x):\n",
    "            x_emb = self.V[x]\n",
    "            return x_emb\n",
    "        \n",
    "    # 2. LSTM\n",
    "    class LSTM:\n",
    "        def __init__(self, in_dim, out_dim, scale, h_0=None, c_0=None):\n",
    "\n",
    "            #- Input gate\n",
    "            self.W_xi = sharedX(rng.randn(in_dim, out_dim)*scale, name='W_xi')\n",
    "            self.W_hi = sharedX(rng.randn(out_dim, out_dim)*scale, name='W_hi')\n",
    "            self.W_ci = sharedX(rng.randn(out_dim, out_dim)*scale, name='W_ci')\n",
    "            self.b_i  = sharedX(rng.randn(out_dim)*scale, name='b_i')\n",
    "\n",
    "            #- Forget gate\n",
    "            self.W_xf = sharedX(rng.randn(in_dim, out_dim)*scale, name='W_xf')\n",
    "            self.W_hf = sharedX(rng.randn(out_dim, out_dim)*scale, name='W_hf')\n",
    "            self.W_cf = sharedX(rng.randn(out_dim, out_dim)*scale, name='W_cf')\n",
    "            self.b_f  = sharedX(rng.randn(out_dim)*scale, name='b_f')\n",
    "\n",
    "            #- Cell state\n",
    "            self.W_xc = sharedX(rng.randn(in_dim, out_dim)*scale, name='W_xc')\n",
    "            self.W_hc = sharedX(rng.randn(out_dim, out_dim)*scale, name='W_hc')\n",
    "            self.b_c  = sharedX(rng.randn(out_dim)*scale, name='b_c')\n",
    "\n",
    "            #- Output gate\n",
    "            self.W_xo = sharedX(rng.randn(in_dim, out_dim)*scale, name='W_xo')\n",
    "            self.W_ho = sharedX(rng.randn(out_dim, out_dim)*scale, name='W_ho')\n",
    "            self.W_co = sharedX(rng.randn(out_dim, out_dim)*scale, name='W_co')\n",
    "            self.b_o  = sharedX(rng.randn(out_dim)*scale, name='b_o')\n",
    "\n",
    "            #- Initial state\n",
    "            if h_0 is None:\n",
    "                self.h_0 = sharedX(np.zeros(out_dim), name='h_0')\n",
    "            else:\n",
    "                self.h_0 = h_0\n",
    "            if c_0 is None:\n",
    "                self.c_0 = sharedX(np.zeros(out_dim), name='c_0')\n",
    "            else:\n",
    "                self.c_0 = c_0\n",
    "\n",
    "            self.output_info = [self.h_0, self.c_0]\n",
    "            self.params = [self.W_xf, self.W_hf, self.W_cf, self.b_f\n",
    "                           , self.W_xi, self.W_hi, self.W_ci, self.b_i\n",
    "                           , self.W_xc, self.W_hc, self.b_c\n",
    "                           , self.W_xo, self.W_ho, self.W_co, self.b_o]\n",
    "\n",
    "        def f_prop(self, x):\n",
    "            def fn(x, h_tm1, c_tm1):\n",
    "                # Input gate\n",
    "                i_t = T.nnet.sigmoid(T.dot(x, self.W_xi) + T.dot(h_tm1, self.W_hi) + T.dot(c_tm1, self.W_ci) \n",
    "                                     + self.b_i)\n",
    "\n",
    "                # Forget gate\n",
    "                f_t = T.nnet.sigmoid(T.dot(x, self.W_xf) + T.dot(h_tm1, self.W_hf) + T.dot(c_tm1, self.W_cf) \n",
    "                                     + self.b_f)\n",
    "\n",
    "                # Cell state\n",
    "                c_t = f_t * c_tm1 + i_t * T.tanh(T.dot(x, self.W_xc) + T.dot(h_tm1, self.W_hc) + self.b_c)\n",
    "\n",
    "                # Output gate\n",
    "                o_t = T.nnet.sigmoid(T.dot(x, self.W_xo) + T.dot(h_tm1, self.W_ho) + T.dot(c_t, self.W_co) \n",
    "                                     + self.b_o)\n",
    "\n",
    "                # Hidden state\n",
    "                h_t = o_t * T.tanh(c_t)\n",
    "\n",
    "                return h_t, c_t\n",
    "\n",
    "            [h,c], _ = theano.scan(fn=fn,\n",
    "                                 sequences=[x],\n",
    "                                 outputs_info=self.output_info)\n",
    "\n",
    "            return h\n",
    "        \n",
    "    # 3. FC-layer\n",
    "    class Linear:\n",
    "        def __init__(self, in_dim, out_dim, scale):\n",
    "            self.W_out = sharedX(rng.randn(in_dim, out_dim)*scale, name='W_out')\n",
    "            self.b_out = sharedX(rng.randn(out_dim,)*scale, name='b_out')\n",
    "            self.params = [self.W_out, self.b_out]\n",
    "\n",
    "        def f_prop(self, x):\n",
    "            z = T.dot(x, self.W_out) + self.b_out\n",
    "            return z\n",
    "        \n",
    "    # 4. activation function\n",
    "    class Activation:\n",
    "        def __init__(self, function):\n",
    "            self.function = function\n",
    "            self.params = []\n",
    "\n",
    "        def f_prop(self, x):\n",
    "            self.z = self.function(x)\n",
    "            return self.z\n",
    "        \n",
    "    # 5. optimization\n",
    "    def sgd(cost, params, eps=np.float32(0.1)):\n",
    "        g_params = T.grad(cost, params)\n",
    "        updates = OrderedDict()\n",
    "        for param, g_param in zip(params, g_params):\n",
    "            updates[param] = param - eps*g_param\n",
    "        return updates\n",
    "    \n",
    "    def Adam(params, g_params, lr=0.001, b1=0.1, b2=0.001, e=1e-8):\n",
    "        updates = []\n",
    "        i = theano.shared(np.float32(0.))\n",
    "        i_t = i + 1.\n",
    "        fix1 = 1. - (1. - b1)**i_t\n",
    "        fix2 = 1. - (1. - b2)**i_t\n",
    "        lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "        for p, g in zip(params, g_params):\n",
    "            m = theano.shared(p.get_value() * 0.)\n",
    "            v = theano.shared(p.get_value() * 0.)\n",
    "            m_t = (b1 * g) + ((1. - b1) * m)\n",
    "            v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "            g_t = m_t / (T.sqrt(v_t) + e)\n",
    "            p_t = p - (lr_t * g_t)\n",
    "            updates.append((m, m_t))\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((p, p_t))\n",
    "        updates.append((i, i_t))\n",
    "        return updates\n",
    "    \n",
    "    # 6. model definition\n",
    "    x = T.ivector('x')\n",
    "    t = T.ivector('t')\n",
    "\n",
    "    # Target\n",
    "    t_in = t[:-1]\n",
    "    t_out = t[1:]\n",
    "\n",
    "    hid_dim = 100\n",
    "\n",
    "    def f_props(layers, x):\n",
    "        layer_out = x\n",
    "        for i, layer in enumerate(layers):\n",
    "            if i == 0:\n",
    "                layer_out = layer.f_prop(x)\n",
    "            else:\n",
    "                layer_out = layer.f_prop(layer_out)\n",
    "        return layer_out\n",
    "\n",
    "    encoder = [\n",
    "        Projection(len(f_vocab), 500, scale=0.01),\n",
    "        LSTM(500, hid_dim, 0.01),\n",
    "    ]\n",
    "\n",
    "    h_enc = f_props(encoder, x)[-1] # Take the last state of encoder\n",
    "\n",
    "    decoder = [\n",
    "        Projection(len(e_vocab), 500, scale=0.01),\n",
    "        LSTM(500, hid_dim, 0.01, h_0=h_enc),\n",
    "        Linear(hid_dim, len(e_vocab), 0.01),\n",
    "        Activation(T.nnet.softmax)\n",
    "    ]\n",
    "    \n",
    "    # 7. compile theano function\n",
    "    def join(layers):\n",
    "        params = []\n",
    "        for layer in layers:\n",
    "            params += layer.params\n",
    "        return params\n",
    "\n",
    "    y = f_props(decoder, t_in)\n",
    "    cost = T.mean(T.nnet.categorical_crossentropy(y, t_out))\n",
    "\n",
    "    params = join(encoder + decoder)\n",
    "    gparams = T.grad(cost, params)\n",
    "    #updates = sgd(cost, params, 0.1)\n",
    "    updates = Adam(params, gparams, lr=0.001, b1=0.1, b2=0.001, e=1e-8)\n",
    "\n",
    "    train = theano.function(inputs=[x, t], outputs=cost, updates=updates)\n",
    "    valid = theano.function(inputs=[x, t], outputs=cost)\n",
    "    \n",
    "    # 8. training loops and validation\n",
    "    max_epoch = 10\n",
    "    batch_size = 1000\n",
    "    import time\n",
    "    for epoch in xrange(max_epoch):\n",
    "        start = time.clock()\n",
    "        train_X, train_y = shuffle(train_X, train_y)  # Shuffle Samples !!\n",
    "        batch_cost = 0\n",
    "        for i, (instance_x, instance_y) in enumerate(zip(train_X, train_y)):\n",
    "            train_cost = train(instance_x, instance_y)\n",
    "            batch_cost += train_cost\n",
    "            if i % batch_size == 0 and i != 0:\n",
    "                print \"EPOCH:: %i, Iteration %i, Training Cost: %.3f\" % (epoch + 1, i, batch_cost / batch_size)\n",
    "                batch_cost = 0\n",
    "            #if (i+1)%5000 == 0:\n",
    "                #break\n",
    "        print \"used time: %.3f\" % (time.clock() - start)\n",
    "        \n",
    "        valid_cost = 0\n",
    "        for i, (instance_x, instance_y) in enumerate(zip(valid_X, valid_y)):\n",
    "            valid_cost += valid(instance_x, instance_y)\n",
    "        print \"EPOCH:: %i, validation cost: %.3f\" % (epoch + 1, valid_cost / len(valid_X))\n",
    "\n",
    "    #- 以下の行はそのままでsubmitしてください (修正しないでください)\n",
    "    test = theano.function(inputs=[x, t], outputs=cost)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "ilect": {
     "course_id": 1,
     "course_rank": 11,
     "is_evaluation": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 4007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:: 1, Iteration 1000, Training Cost: 5.186\n",
      "EPOCH:: 1, Iteration 2000, Training Cost: 4.457\n",
      "EPOCH:: 1, Iteration 3000, Training Cost: 4.234\n",
      "EPOCH:: 1, Iteration 4000, Training Cost: 4.029\n",
      "EPOCH:: 1, Iteration 5000, Training Cost: 3.922\n",
      "EPOCH:: 1, Iteration 6000, Training Cost: 3.847\n",
      "EPOCH:: 1, Iteration 7000, Training Cost: 3.714\n",
      "EPOCH:: 1, Iteration 8000, Training Cost: 3.678\n",
      "EPOCH:: 1, Iteration 9000, Training Cost: 3.622\n",
      "EPOCH:: 1, Iteration 10000, Training Cost: 3.566\n",
      "EPOCH:: 1, Iteration 11000, Training Cost: 3.544\n",
      "EPOCH:: 1, Iteration 12000, Training Cost: 3.576\n",
      "EPOCH:: 1, Iteration 13000, Training Cost: 3.502\n",
      "EPOCH:: 1, Iteration 14000, Training Cost: 3.456\n",
      "EPOCH:: 1, Iteration 15000, Training Cost: 3.441\n",
      "EPOCH:: 1, Iteration 16000, Training Cost: 3.373\n",
      "EPOCH:: 1, Iteration 17000, Training Cost: 3.385\n",
      "EPOCH:: 1, Iteration 18000, Training Cost: 3.339\n",
      "EPOCH:: 1, Iteration 19000, Training Cost: 3.341\n",
      "EPOCH:: 1, Iteration 20000, Training Cost: 3.314\n",
      "EPOCH:: 1, Iteration 21000, Training Cost: 3.286\n",
      "EPOCH:: 1, Iteration 22000, Training Cost: 3.296\n",
      "EPOCH:: 1, Iteration 23000, Training Cost: 3.268\n",
      "EPOCH:: 1, Iteration 24000, Training Cost: 3.219\n",
      "EPOCH:: 1, Iteration 25000, Training Cost: 3.223\n",
      "EPOCH:: 1, Iteration 26000, Training Cost: 3.251\n",
      "EPOCH:: 1, Iteration 27000, Training Cost: 3.209\n",
      "EPOCH:: 1, Iteration 28000, Training Cost: 3.231\n",
      "used time: 499.261\n",
      "EPOCH:: 1, validation cost: 3.186\n",
      "EPOCH:: 2, Iteration 1000, Training Cost: 3.036\n",
      "EPOCH:: 2, Iteration 2000, Training Cost: 3.030\n",
      "EPOCH:: 2, Iteration 3000, Training Cost: 3.024\n",
      "EPOCH:: 2, Iteration 4000, Training Cost: 2.954\n",
      "EPOCH:: 2, Iteration 5000, Training Cost: 3.050\n",
      "EPOCH:: 2, Iteration 6000, Training Cost: 3.035\n",
      "EPOCH:: 2, Iteration 7000, Training Cost: 3.049\n",
      "EPOCH:: 2, Iteration 8000, Training Cost: 3.007\n",
      "EPOCH:: 2, Iteration 9000, Training Cost: 3.037\n",
      "EPOCH:: 2, Iteration 10000, Training Cost: 2.993\n",
      "EPOCH:: 2, Iteration 11000, Training Cost: 3.041\n",
      "EPOCH:: 2, Iteration 12000, Training Cost: 2.987\n",
      "EPOCH:: 2, Iteration 13000, Training Cost: 2.995\n",
      "EPOCH:: 2, Iteration 14000, Training Cost: 3.025\n",
      "EPOCH:: 2, Iteration 15000, Training Cost: 3.059\n",
      "EPOCH:: 2, Iteration 16000, Training Cost: 2.944\n",
      "EPOCH:: 2, Iteration 17000, Training Cost: 2.982\n",
      "EPOCH:: 2, Iteration 18000, Training Cost: 3.059\n",
      "EPOCH:: 2, Iteration 19000, Training Cost: 2.969\n",
      "EPOCH:: 2, Iteration 20000, Training Cost: 2.949\n",
      "EPOCH:: 2, Iteration 21000, Training Cost: 2.936\n",
      "EPOCH:: 2, Iteration 22000, Training Cost: 2.968\n",
      "EPOCH:: 2, Iteration 23000, Training Cost: 2.959\n",
      "EPOCH:: 2, Iteration 24000, Training Cost: 2.908\n",
      "EPOCH:: 2, Iteration 25000, Training Cost: 2.960\n",
      "EPOCH:: 2, Iteration 26000, Training Cost: 2.929\n",
      "EPOCH:: 2, Iteration 27000, Training Cost: 2.925\n",
      "EPOCH:: 2, Iteration 28000, Training Cost: 2.942\n",
      "used time: 497.270\n",
      "EPOCH:: 2, validation cost: 2.996\n",
      "EPOCH:: 3, Iteration 1000, Training Cost: 2.805\n",
      "EPOCH:: 3, Iteration 2000, Training Cost: 2.816\n",
      "EPOCH:: 3, Iteration 3000, Training Cost: 2.711\n",
      "EPOCH:: 3, Iteration 4000, Training Cost: 2.769\n",
      "EPOCH:: 3, Iteration 5000, Training Cost: 2.823\n",
      "EPOCH:: 3, Iteration 6000, Training Cost: 2.776\n",
      "EPOCH:: 3, Iteration 7000, Training Cost: 2.759\n",
      "EPOCH:: 3, Iteration 8000, Training Cost: 2.836\n",
      "EPOCH:: 3, Iteration 9000, Training Cost: 2.722\n",
      "EPOCH:: 3, Iteration 10000, Training Cost: 2.776\n",
      "EPOCH:: 3, Iteration 11000, Training Cost: 2.832\n",
      "EPOCH:: 3, Iteration 12000, Training Cost: 2.786\n",
      "EPOCH:: 3, Iteration 13000, Training Cost: 2.743\n",
      "EPOCH:: 3, Iteration 14000, Training Cost: 2.875\n",
      "EPOCH:: 3, Iteration 15000, Training Cost: 2.713\n",
      "EPOCH:: 3, Iteration 16000, Training Cost: 2.794\n",
      "EPOCH:: 3, Iteration 17000, Training Cost: 2.901\n",
      "EPOCH:: 3, Iteration 18000, Training Cost: 2.826\n",
      "EPOCH:: 3, Iteration 19000, Training Cost: 2.839\n",
      "EPOCH:: 3, Iteration 20000, Training Cost: 2.870\n",
      "EPOCH:: 3, Iteration 21000, Training Cost: 2.808\n",
      "EPOCH:: 3, Iteration 22000, Training Cost: 2.780\n",
      "EPOCH:: 3, Iteration 23000, Training Cost: 2.792\n",
      "EPOCH:: 3, Iteration 24000, Training Cost: 2.750\n",
      "EPOCH:: 3, Iteration 25000, Training Cost: 2.848\n",
      "EPOCH:: 3, Iteration 26000, Training Cost: 2.825\n",
      "EPOCH:: 3, Iteration 27000, Training Cost: 2.796\n",
      "EPOCH:: 3, Iteration 28000, Training Cost: 2.791\n",
      "used time: 497.298\n",
      "EPOCH:: 3, validation cost: 2.924\n",
      "EPOCH:: 4, Iteration 1000, Training Cost: 2.568\n",
      "EPOCH:: 4, Iteration 2000, Training Cost: 2.620\n",
      "EPOCH:: 4, Iteration 3000, Training Cost: 2.653\n",
      "EPOCH:: 4, Iteration 4000, Training Cost: 2.626\n",
      "EPOCH:: 4, Iteration 5000, Training Cost: 2.669\n",
      "EPOCH:: 4, Iteration 6000, Training Cost: 2.702\n",
      "EPOCH:: 4, Iteration 7000, Training Cost: 2.653\n",
      "EPOCH:: 4, Iteration 8000, Training Cost: 2.642\n",
      "EPOCH:: 4, Iteration 9000, Training Cost: 2.689\n",
      "EPOCH:: 4, Iteration 10000, Training Cost: 2.732\n",
      "EPOCH:: 4, Iteration 11000, Training Cost: 2.669\n",
      "EPOCH:: 4, Iteration 12000, Training Cost: 2.753\n",
      "EPOCH:: 4, Iteration 13000, Training Cost: 2.683\n",
      "EPOCH:: 4, Iteration 14000, Training Cost: 2.653\n",
      "EPOCH:: 4, Iteration 15000, Training Cost: 2.695\n",
      "EPOCH:: 4, Iteration 16000, Training Cost: 2.739\n",
      "EPOCH:: 4, Iteration 17000, Training Cost: 2.751\n",
      "EPOCH:: 4, Iteration 18000, Training Cost: 2.727\n",
      "EPOCH:: 4, Iteration 19000, Training Cost: 2.714\n",
      "EPOCH:: 4, Iteration 20000, Training Cost: 2.656\n",
      "EPOCH:: 4, Iteration 21000, Training Cost: 2.716\n",
      "EPOCH:: 4, Iteration 22000, Training Cost: 2.714\n",
      "EPOCH:: 4, Iteration 23000, Training Cost: 2.760\n",
      "EPOCH:: 4, Iteration 24000, Training Cost: 2.750\n",
      "EPOCH:: 4, Iteration 25000, Training Cost: 2.682\n",
      "EPOCH:: 4, Iteration 26000, Training Cost: 2.735\n",
      "EPOCH:: 4, Iteration 27000, Training Cost: 2.736\n",
      "EPOCH:: 4, Iteration 28000, Training Cost: 2.714\n",
      "used time: 498.419\n",
      "EPOCH:: 4, validation cost: 2.888\n",
      "EPOCH:: 5, Iteration 1000, Training Cost: 2.536\n",
      "EPOCH:: 5, Iteration 2000, Training Cost: 2.525\n",
      "EPOCH:: 5, Iteration 3000, Training Cost: 2.575\n",
      "EPOCH:: 5, Iteration 4000, Training Cost: 2.594\n",
      "EPOCH:: 5, Iteration 5000, Training Cost: 2.559\n",
      "EPOCH:: 5, Iteration 6000, Training Cost: 2.594\n",
      "EPOCH:: 5, Iteration 7000, Training Cost: 2.552\n",
      "EPOCH:: 5, Iteration 8000, Training Cost: 2.602\n",
      "EPOCH:: 5, Iteration 9000, Training Cost: 2.590\n",
      "EPOCH:: 5, Iteration 10000, Training Cost: 2.665\n",
      "EPOCH:: 5, Iteration 11000, Training Cost: 2.620\n",
      "EPOCH:: 5, Iteration 12000, Training Cost: 2.599\n",
      "EPOCH:: 5, Iteration 13000, Training Cost: 2.649\n",
      "EPOCH:: 5, Iteration 14000, Training Cost: 2.685\n",
      "EPOCH:: 5, Iteration 15000, Training Cost: 2.622\n",
      "EPOCH:: 5, Iteration 16000, Training Cost: 2.690\n",
      "EPOCH:: 5, Iteration 17000, Training Cost: 2.593\n",
      "EPOCH:: 5, Iteration 18000, Training Cost: 2.625\n",
      "EPOCH:: 5, Iteration 19000, Training Cost: 2.675\n",
      "EPOCH:: 5, Iteration 20000, Training Cost: 2.689\n",
      "EPOCH:: 5, Iteration 21000, Training Cost: 2.711\n",
      "EPOCH:: 5, Iteration 22000, Training Cost: 2.626\n",
      "EPOCH:: 5, Iteration 23000, Training Cost: 2.652\n",
      "EPOCH:: 5, Iteration 24000, Training Cost: 2.628\n",
      "EPOCH:: 5, Iteration 25000, Training Cost: 2.696\n",
      "EPOCH:: 5, Iteration 26000, Training Cost: 2.627\n",
      "EPOCH:: 5, Iteration 27000, Training Cost: 2.666\n",
      "EPOCH:: 5, Iteration 28000, Training Cost: 2.689\n",
      "used time: 497.749\n",
      "EPOCH:: 5, validation cost: 2.879\n",
      "EPOCH:: 6, Iteration 1000, Training Cost: 2.411\n",
      "EPOCH:: 6, Iteration 2000, Training Cost: 2.540\n",
      "EPOCH:: 6, Iteration 3000, Training Cost: 2.487\n",
      "EPOCH:: 6, Iteration 4000, Training Cost: 2.532\n",
      "EPOCH:: 6, Iteration 5000, Training Cost: 2.526\n",
      "EPOCH:: 6, Iteration 6000, Training Cost: 2.492\n",
      "EPOCH:: 6, Iteration 7000, Training Cost: 2.541\n",
      "EPOCH:: 6, Iteration 8000, Training Cost: 2.563\n",
      "EPOCH:: 6, Iteration 9000, Training Cost: 2.519\n",
      "EPOCH:: 6, Iteration 10000, Training Cost: 2.585\n",
      "EPOCH:: 6, Iteration 11000, Training Cost: 2.612\n",
      "EPOCH:: 6, Iteration 12000, Training Cost: 2.622\n",
      "EPOCH:: 6, Iteration 13000, Training Cost: 2.595\n",
      "EPOCH:: 6, Iteration 14000, Training Cost: 2.593\n",
      "EPOCH:: 6, Iteration 15000, Training Cost: 2.564\n",
      "EPOCH:: 6, Iteration 16000, Training Cost: 2.568\n",
      "EPOCH:: 6, Iteration 17000, Training Cost: 2.638\n",
      "EPOCH:: 6, Iteration 18000, Training Cost: 2.617\n",
      "EPOCH:: 6, Iteration 19000, Training Cost: 2.612\n",
      "EPOCH:: 6, Iteration 20000, Training Cost: 2.609\n",
      "EPOCH:: 6, Iteration 21000, Training Cost: 2.609\n",
      "EPOCH:: 6, Iteration 22000, Training Cost: 2.575\n",
      "EPOCH:: 6, Iteration 23000, Training Cost: 2.601\n",
      "EPOCH:: 6, Iteration 24000, Training Cost: 2.609\n",
      "EPOCH:: 6, Iteration 25000, Training Cost: 2.644\n",
      "EPOCH:: 6, Iteration 26000, Training Cost: 2.596\n",
      "EPOCH:: 6, Iteration 27000, Training Cost: 2.647\n",
      "EPOCH:: 6, Iteration 28000, Training Cost: 2.657\n",
      "used time: 498.327\n",
      "EPOCH:: 6, validation cost: 2.855\n",
      "EPOCH:: 7, Iteration 1000, Training Cost: 2.407\n",
      "EPOCH:: 7, Iteration 2000, Training Cost: 2.429\n",
      "EPOCH:: 7, Iteration 3000, Training Cost: 2.400\n",
      "EPOCH:: 7, Iteration 4000, Training Cost: 2.464\n",
      "EPOCH:: 7, Iteration 5000, Training Cost: 2.511\n",
      "EPOCH:: 7, Iteration 6000, Training Cost: 2.455\n",
      "EPOCH:: 7, Iteration 7000, Training Cost: 2.506\n",
      "EPOCH:: 7, Iteration 8000, Training Cost: 2.538\n",
      "EPOCH:: 7, Iteration 9000, Training Cost: 2.531\n",
      "EPOCH:: 7, Iteration 10000, Training Cost: 2.564\n",
      "EPOCH:: 7, Iteration 11000, Training Cost: 2.562\n",
      "EPOCH:: 7, Iteration 12000, Training Cost: 2.605\n",
      "EPOCH:: 7, Iteration 13000, Training Cost: 2.549\n",
      "EPOCH:: 7, Iteration 14000, Training Cost: 2.572\n",
      "EPOCH:: 7, Iteration 15000, Training Cost: 2.502\n",
      "EPOCH:: 7, Iteration 16000, Training Cost: 2.589\n",
      "EPOCH:: 7, Iteration 17000, Training Cost: 2.564\n",
      "EPOCH:: 7, Iteration 18000, Training Cost: 2.606\n",
      "EPOCH:: 7, Iteration 19000, Training Cost: 2.591\n",
      "EPOCH:: 7, Iteration 20000, Training Cost: 2.578\n",
      "EPOCH:: 7, Iteration 21000, Training Cost: 2.626\n",
      "EPOCH:: 7, Iteration 22000, Training Cost: 2.547\n",
      "EPOCH:: 7, Iteration 23000, Training Cost: 2.663\n",
      "EPOCH:: 7, Iteration 24000, Training Cost: 2.606\n",
      "EPOCH:: 7, Iteration 25000, Training Cost: 2.604\n",
      "EPOCH:: 7, Iteration 26000, Training Cost: 2.534\n",
      "EPOCH:: 7, Iteration 27000, Training Cost: 2.643\n",
      "EPOCH:: 7, Iteration 28000, Training Cost: 2.579\n",
      "used time: 497.008\n",
      "EPOCH:: 7, validation cost: 2.850\n",
      "EPOCH:: 8, Iteration 1000, Training Cost: 2.432\n",
      "EPOCH:: 8, Iteration 2000, Training Cost: 2.482\n",
      "EPOCH:: 8, Iteration 3000, Training Cost: 2.441\n",
      "EPOCH:: 8, Iteration 4000, Training Cost: 2.505\n",
      "EPOCH:: 8, Iteration 5000, Training Cost: 2.427\n",
      "EPOCH:: 8, Iteration 6000, Training Cost: 2.466\n",
      "EPOCH:: 8, Iteration 7000, Training Cost: 2.505\n",
      "EPOCH:: 8, Iteration 8000, Training Cost: 2.457\n",
      "EPOCH:: 8, Iteration 9000, Training Cost: 2.494\n",
      "EPOCH:: 8, Iteration 10000, Training Cost: 2.518\n",
      "EPOCH:: 8, Iteration 11000, Training Cost: 2.479\n",
      "EPOCH:: 8, Iteration 12000, Training Cost: 2.549\n",
      "EPOCH:: 8, Iteration 13000, Training Cost: 2.545\n",
      "EPOCH:: 8, Iteration 14000, Training Cost: 2.486\n",
      "EPOCH:: 8, Iteration 15000, Training Cost: 2.510\n",
      "EPOCH:: 8, Iteration 16000, Training Cost: 2.547\n",
      "EPOCH:: 8, Iteration 17000, Training Cost: 2.585\n",
      "EPOCH:: 8, Iteration 18000, Training Cost: 2.516\n",
      "EPOCH:: 8, Iteration 19000, Training Cost: 2.602\n",
      "EPOCH:: 8, Iteration 20000, Training Cost: 2.520\n",
      "EPOCH:: 8, Iteration 21000, Training Cost: 2.594\n",
      "EPOCH:: 8, Iteration 22000, Training Cost: 2.522\n",
      "EPOCH:: 8, Iteration 23000, Training Cost: 2.584\n",
      "EPOCH:: 8, Iteration 24000, Training Cost: 2.541\n",
      "EPOCH:: 8, Iteration 25000, Training Cost: 2.546\n",
      "EPOCH:: 8, Iteration 26000, Training Cost: 2.584\n",
      "EPOCH:: 8, Iteration 27000, Training Cost: 2.607\n",
      "EPOCH:: 8, Iteration 28000, Training Cost: 2.623\n",
      "used time: 497.303\n",
      "EPOCH:: 8, validation cost: 2.859\n",
      "EPOCH:: 9, Iteration 1000, Training Cost: 2.395\n",
      "EPOCH:: 9, Iteration 2000, Training Cost: 2.382\n",
      "EPOCH:: 9, Iteration 3000, Training Cost: 2.423\n",
      "EPOCH:: 9, Iteration 4000, Training Cost: 2.418\n",
      "EPOCH:: 9, Iteration 5000, Training Cost: 2.442\n",
      "EPOCH:: 9, Iteration 6000, Training Cost: 2.488\n",
      "EPOCH:: 9, Iteration 7000, Training Cost: 2.464\n",
      "EPOCH:: 9, Iteration 8000, Training Cost: 2.483\n",
      "EPOCH:: 9, Iteration 9000, Training Cost: 2.489\n",
      "EPOCH:: 9, Iteration 10000, Training Cost: 2.464\n",
      "EPOCH:: 9, Iteration 11000, Training Cost: 2.461\n",
      "EPOCH:: 9, Iteration 12000, Training Cost: 2.510\n",
      "EPOCH:: 9, Iteration 13000, Training Cost: 2.440\n",
      "EPOCH:: 9, Iteration 14000, Training Cost: 2.477\n",
      "EPOCH:: 9, Iteration 15000, Training Cost: 2.439\n",
      "EPOCH:: 9, Iteration 16000, Training Cost: 2.539\n",
      "EPOCH:: 9, Iteration 17000, Training Cost: 2.456\n",
      "EPOCH:: 9, Iteration 18000, Training Cost: 2.573\n",
      "EPOCH:: 9, Iteration 19000, Training Cost: 2.601\n",
      "EPOCH:: 9, Iteration 20000, Training Cost: 2.531\n",
      "EPOCH:: 9, Iteration 21000, Training Cost: 2.546\n",
      "EPOCH:: 9, Iteration 22000, Training Cost: 2.527\n",
      "EPOCH:: 9, Iteration 23000, Training Cost: 2.564\n",
      "EPOCH:: 9, Iteration 24000, Training Cost: 2.580\n",
      "EPOCH:: 9, Iteration 25000, Training Cost: 2.549\n",
      "EPOCH:: 9, Iteration 26000, Training Cost: 2.557\n",
      "EPOCH:: 9, Iteration 27000, Training Cost: 2.558\n",
      "EPOCH:: 9, Iteration 28000, Training Cost: 2.599\n",
      "used time: 497.984\n",
      "EPOCH:: 9, validation cost: 2.855\n",
      "EPOCH:: 10, Iteration 1000, Training Cost: 2.407\n",
      "EPOCH:: 10, Iteration 2000, Training Cost: 2.411\n",
      "EPOCH:: 10, Iteration 3000, Training Cost: 2.459\n",
      "EPOCH:: 10, Iteration 4000, Training Cost: 2.368\n",
      "EPOCH:: 10, Iteration 5000, Training Cost: 2.392\n",
      "EPOCH:: 10, Iteration 6000, Training Cost: 2.470\n",
      "EPOCH:: 10, Iteration 7000, Training Cost: 2.493\n",
      "EPOCH:: 10, Iteration 8000, Training Cost: 2.441\n",
      "EPOCH:: 10, Iteration 9000, Training Cost: 2.438\n",
      "EPOCH:: 10, Iteration 10000, Training Cost: 2.525\n",
      "EPOCH:: 10, Iteration 11000, Training Cost: 2.481\n",
      "EPOCH:: 10, Iteration 12000, Training Cost: 2.479\n",
      "EPOCH:: 10, Iteration 13000, Training Cost: 2.496\n",
      "EPOCH:: 10, Iteration 14000, Training Cost: 2.479\n",
      "EPOCH:: 10, Iteration 15000, Training Cost: 2.497\n",
      "EPOCH:: 10, Iteration 16000, Training Cost: 2.455\n",
      "EPOCH:: 10, Iteration 17000, Training Cost: 2.541\n",
      "EPOCH:: 10, Iteration 18000, Training Cost: 2.492\n",
      "EPOCH:: 10, Iteration 19000, Training Cost: 2.535\n",
      "EPOCH:: 10, Iteration 20000, Training Cost: 2.542\n",
      "EPOCH:: 10, Iteration 21000, Training Cost: 2.524\n",
      "EPOCH:: 10, Iteration 22000, Training Cost: 2.485\n",
      "EPOCH:: 10, Iteration 23000, Training Cost: 2.484\n",
      "EPOCH:: 10, Iteration 24000, Training Cost: 2.505\n",
      "EPOCH:: 10, Iteration 25000, Training Cost: 2.570\n",
      "EPOCH:: 10, Iteration 26000, Training Cost: 2.542\n",
      "EPOCH:: 10, Iteration 27000, Training Cost: 2.511\n",
      "EPOCH:: 10, Iteration 28000, Training Cost: 2.573\n",
      "used time: 496.949\n",
      "EPOCH:: 10, validation cost: 2.874\n",
      "2.88889\n",
      "No Error Occured!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "trng = RandomStreams(42)\n",
    "\n",
    "def build_vocab(file_path):\n",
    "    f_vocab, e_vocab = set(), set()\n",
    "    for line in open(file_path):\n",
    "        f, e = [l.strip().split()[1:-1] for l in line.split('|||')]\n",
    "        f_vocab.update(f)\n",
    "        e_vocab.update(e)\n",
    "    \n",
    "    f_w2i = {w: np.int32(i+2) for i, w in enumerate(f_vocab)}\n",
    "    e_w2i = {w: np.int32(i+2) for i, w in enumerate(e_vocab)}\n",
    "    \n",
    "    f_w2i['<s>'], f_w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "    e_w2i['<s>'], e_w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "    return set(f_w2i.keys()), set(e_w2i.keys()), f_w2i, e_w2i\n",
    "    \n",
    "def encode(sentence, vocab, w2i):\n",
    "    encoded_sentence = []\n",
    "    for w in sentence:\n",
    "        if w in vocab:\n",
    "            encoded_sentence.append(w2i[w])\n",
    "        else:\n",
    "            encoded_sentence.append(w2i['UNK'])\n",
    "    return encoded_sentence\n",
    "    \n",
    "def decode(encoded_sentence, w2i):\n",
    "    i2w = {i:w for w, i in w2i.items()}\n",
    "    decoded_sentence = []\n",
    "    for i in encoded_sentence:\n",
    "        decoded_sentence.append(i2w[i])\n",
    "    return decoded_sentence\n",
    "    \n",
    "def load_data(file_path, f_vocab, e_vocab, f_w2i, e_w2i):\n",
    "    x, y = [], []\n",
    "    for line in open(file_path):\n",
    "        f, e = [l.strip().split() for l in line.split('|||')]\n",
    "        f_enc = encode(f, f_vocab, f_w2i)\n",
    "        e_enc = encode(e, e_vocab, e_w2i)\n",
    "        x.append(f_enc)\n",
    "        y.append(e_enc)\n",
    "    return x, y\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    global f_vocab\n",
    "    global e_vocab\n",
    "    \n",
    "    f_vocab, e_vocab, f_w2i, e_w2i = build_vocab(dataset_path)\n",
    "    train_X, train_y = load_data(dataset_path, f_vocab, e_vocab, f_w2i, e_w2i)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y\n",
    "    \n",
    "def check_homework():\n",
    "    train_X, test_X, train_y, test_y = load_dataset('./train.zh-en')\n",
    "    test = homework(train_X, train_y)\n",
    "    crs_ent_list = [test(ins_x, ins_y) for ins_x, ins_y in zip(test_X, test_y)]\n",
    "\n",
    "    return np.array(crs_ent_list).mean()\n",
    "\n",
    "if 'homework' in globals():\n",
    "    result = check_homework()\n",
    "    print result\n",
    "    print \"No Error Occured!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
