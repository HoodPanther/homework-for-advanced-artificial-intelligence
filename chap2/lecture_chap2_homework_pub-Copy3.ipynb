{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第2回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題. MNISTをk-NNで学習せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意\n",
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データはtrain_X, train_y, テストデータはtest_Xで与えられます\n",
    "    - train_Xとtrain_yをtrain_X, train_yとvalid_X, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - test_Xに対して予想ラベルpred_yを作り, homework関数の戻り値としてください\\\n",
    "- pred_yのtest_yに対する精度(F値)で評価します\n",
    "- 全体の実行時間がiLect上で60分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-NNの実装にscikit-learnなどのライブラリを使わないでください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のコードが**事前**に実行されます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "from __future__ import division\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X, mnist_y = shuffle(mnist.data, mnist.target.astype('int32'))\n",
    "\n",
    "mnist_X = mnist_X / 255.0\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y, test_size=0.2, random_state=??) # random_stateはこちらで与えます\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルのhomework関数を完成させて提出してください\n",
    "- パッケージのインポートなども含めて、必要な物はすべて書いてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    # WRITE ME\n",
    "    from sklearn.base import BaseEstimator\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import bottleneck\n",
    "    \n",
    "    class knn_estimator(BaseEstimator):\n",
    "        \"\"\"k-nearest-neighbor classifier.\"\"\"\n",
    "        def __init__(self, k=1, sub_size=1000):\n",
    "            self.k = k\n",
    "            self.sub_size = sub_size\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            # for the memory constriction, we split the testing set into small subsets\n",
    "            sub_counts = int(np.ceil(X.shape[0] / self.sub_size))\n",
    "            Xs = np.array_split(X, sub_counts)\n",
    "            pred_y = []\n",
    "            k = self.k\n",
    "            for i in xrange(sub_counts):\n",
    "                tmp_pred_y = np.zeros(Xs[i].shape[0])\n",
    "                similarities = cosine_similarity(Xs[i], self.X)\n",
    "                for j in xrange(Xs[i].shape[0]):\n",
    "                    # candidates are 10 digits, i.e., 0,1,2,...,8,9\n",
    "                    candidates = np.zeros(10)\n",
    "                    # bottleneck partially sort the similarities, so we get k indexes with largest similarities in training set\n",
    "                    knns = bottleneck.argpartsort(-similarities[j],n = k)[:k]\n",
    "                    # voting for pred_y\n",
    "                    for nn in knns:\n",
    "                        candidates[train_y[nn]] += 1\n",
    "                    # get pred_y by majority voting\n",
    "                    tmp_pred_y[j] = np.argmax(candidates)\n",
    "                pred_y.append(tmp_pred_y)\n",
    "            pred_y = np.hstack(pred_y)\n",
    "            return pred_y\n",
    "    \n",
    "    knn = knn_estimator(k=5, sub_size=1000)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X, mnist_y = shuffle(mnist.data, mnist.target.astype('int32'))\n",
    "\n",
    "mnist_X = mnist_X / 255.0\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y, test_size=0.2)\n",
    "\n",
    "def check_homework():\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    return f1_score(test_y, pred_y, average='macro')\n",
    "\n",
    "if 'homework' in globals():\n",
    "    result = check_homework()\n",
    "    print(result)\n",
    "    if result > 0.95:\n",
    "        print \"No Error Occured! Your F1 score is 0.95~1.00\"\n",
    "    elif result > 0.90:\n",
    "        print \"No Error Occured! Your F1 score is 0.90~0.95\"\n",
    "    else:\n",
    "        print \"No Error Occured! Your F1 score is under 0.90\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X, mnist_y = shuffle(mnist.data, mnist.target.astype('int32'))\n",
    "\n",
    "mnist_X = mnist_X / 255.0\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y, test_size=0.2)\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import bottleneck\n",
    "class knn_estimator(BaseEstimator):\n",
    "    \"\"\"k-nearest-neighbor classifier.\"\"\"\n",
    "    def __init__(self, k=1, sub_size=1000):\n",
    "        self.k = k\n",
    "        self.sub_size = sub_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sub_counts = int(np.ceil(X.shape[0] / self.sub_size))\n",
    "        Xs = np.array_split(X, sub_counts)\n",
    "        pred_y = []\n",
    "        k = self.k\n",
    "        for i in xrange(sub_counts):\n",
    "            tmp_pred_y = np.zeros(Xs[i].shape[0])\n",
    "            similarities = cosine_similarity(Xs[i], self.X)\n",
    "            for j in xrange(Xs[i].shape[0]):\n",
    "                # candidates are 10 digits, i.e., 0,1,2,...,8,9\n",
    "                candidates = np.zeros(10)\n",
    "                # bottleneck partially sort the similarities, so we get k indexes with largest similarities in training set\n",
    "                knns = bottleneck.argpartsort(-similarities[j],n = k)[:k]\n",
    "                # voting for pred_y\n",
    "                for nn in knns:\n",
    "                    candidates[train_y[nn]] += 1\n",
    "                # get pred_y by majority voting\n",
    "                tmp_pred_y[j] = np.argmax(candidates)\n",
    "            pred_y.append(tmp_pred_y)\n",
    "        pred_y = np.hstack(pred_y)\n",
    "        return pred_y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn1 = knn_estimator(5, 1000)\n",
    "knn1.fit(train_X, train_y)\n",
    "pred_y = knn1.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "TypeError not raised by fit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4878fe95431c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_checks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcheck_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknn_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/utils/estimator_checks.pyc\u001b[0m in \u001b[0;36mcheck_estimator\u001b[1;34m(Estimator)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[0mcheck_parameters_default_constructible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_yield_all_checks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/utils/estimator_checks.pyc\u001b[0m in \u001b[0;36mcheck_dtype_object\u001b[1;34m(name, Estimator)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'foo'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bar'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"argument must be a string or a number\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m     \u001b[0massert_raises_regex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/utils/testing.pyc\u001b[0m in \u001b[0;36massert_raises_regex\u001b[1;34m(expected_exception, expected_regexp, callable_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             raise AssertionError(\"%s not raised by %s\" %\n\u001b[0;32m    109\u001b[0m                                  (expected_exception.__name__,\n\u001b[1;32m--> 110\u001b[1;33m                                   callable_obj.__name__))\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;31m# assert_raises_regexp is deprecated in Python 3.4 in favor of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: TypeError not raised by fit"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "check_estimator(knn_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'k': [1,3,5,10]},\n",
    " ]\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "clf = GridSearchCV(knn_estimator(), param_grid, cv=5,\n",
    "                       scoring='f1_macro')\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ilect": {
     "is_homework": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 10}\n",
      "0.532 (+/-0.617) for {'k': 1}\n",
      "0.588 (+/-0.654) for {'k': 3}\n",
      "0.657 (+/-0.671) for {'k': 5}\n",
      "0.716 (+/-0.677) for {'k': 10}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        % (mean_score, scores.std() * 2, params))\n",
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ilect": {
     "course_id": 1,
     "course_rank": 2,
     "is_evaluation": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X, mnist_y = shuffle(mnist.data, mnist.target.astype('int32'))\n",
    "\n",
    "mnist_X = mnist_X / 255.0\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y, test_size=0.2)\n",
    "\n",
    "def check_homework():\n",
    "    pred_y = homework(train_X, train_y, test_X)\n",
    "    return f1_score(test_y, pred_y, average='macro')\n",
    "\n",
    "if 'homework' in globals():\n",
    "    result = check_homework()\n",
    "    \n",
    "    if result > 0.95:\n",
    "        print \"No Error Occured! Your F1 score is 0.95~1.00\"\n",
    "    elif result > 0.90:\n",
    "        print \"No Error Occured! Your F1 score is 0.90~0.95\"\n",
    "    else:\n",
    "        print \"No Error Occured! Your F1 score is under 0.90\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
