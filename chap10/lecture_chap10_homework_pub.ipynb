{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題. RNNを用いてPOS taggingを実装せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データはtrain_X, train_y, テストデータはtest_Xで与えられます\n",
    "    - train_Xとtrain_yをtrain_X, train_yとvalid_X, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - test_Xに対して予想ラベルpred_yを作り, homework関数の戻り値としてください\n",
    "    - pred_yは1次元のlistとしてください\n",
    "- pred_yのtest_yに対する精度(F値)で評価します\n",
    "- 全体の実行時間がiLect上で60分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のような内容のコードが事前に実行されます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from __future__ import division\n",
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "trng = RandomStreams(42)\n",
    "\n",
    "def load_data(file_path):\n",
    "    dataset = []\n",
    "    vocab, tag = set(), set()\n",
    "    for line in open(file_path):\n",
    "        instance = [l.strip().split() for l in line.split('|||')]\n",
    "        vocab.update(instance[0])\n",
    "        tag.update(instance[1])\n",
    "        dataset.append(instance)\n",
    "    return dataset, vocab, tag\n",
    "\n",
    "def encode_dataset(dataset, word2index, tag2index):\n",
    "    X, y = [], []\n",
    "    vocab = set(word2index.keys())\n",
    "    for sentence, tags in dataset:\n",
    "        X.append([word2index[word] if word in vocab else word2index['<unk>'] for word in sentence])\n",
    "        y.append([tag2index[tag] for tag in tags])\n",
    "    return X, y\n",
    "\n",
    "train_data, train_vocab, train_tags = load_data('train.unk')\n",
    "special_words = set(['<unk>'])\n",
    "\n",
    "global word2index\n",
    "global tag2index\n",
    "\n",
    "word2index = dict(map(lambda x: (x[1], x[0]), enumerate(train_vocab | special_words)))\n",
    "tag2index  = dict(map(lambda x: (x[1], x[0]), enumerate(train_tags)))\n",
    "\n",
    "train_X, train_y = encode_dataset(train_data, word2index, tag2index)\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.2, random_state=??) # random_stateはひみつです\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルのhomework関数を完成させて提出してください\n",
    "- **上記のコード以外で必要なもの**は全て書いてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "trng = RandomStreams(42)\n",
    "\n",
    "def load_data(file_path):\n",
    "    dataset = []\n",
    "    vocab, tag = set(), set()\n",
    "    for line in open(file_path):\n",
    "        instance = [l.strip().split() for l in line.split('|||')]\n",
    "        vocab.update(instance[0])\n",
    "        tag.update(instance[1])\n",
    "        dataset.append(instance)\n",
    "    return dataset, vocab, tag\n",
    "\n",
    "def encode_dataset(dataset, word2index, tag2index):\n",
    "    X, y = [], []\n",
    "    vocab = set(word2index.keys())\n",
    "    for sentence, tags in dataset:\n",
    "        X.append([word2index[word] if word in vocab else word2index['<unk>'] for word in sentence])\n",
    "        y.append([tag2index[tag] for tag in tags])\n",
    "    return X, y\n",
    "\n",
    "def load_dataset():\n",
    "    train_data, train_vocab, train_tags = load_data('train.unk')\n",
    "    special_words = set(['<unk>'])\n",
    "    \n",
    "    global word2index\n",
    "    global tag2index\n",
    "\n",
    "    word2index = dict(map(lambda x: (x[1], x[0]), enumerate(train_vocab | special_words)))\n",
    "    tag2index  = dict(map(lambda x: (x[1], x[0]), enumerate(train_tags)))\n",
    "\n",
    "    train_X, train_y = encode_dataset(train_data, word2index, tag2index)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return (train_X, test_X, train_y, test_y)\n",
    "\n",
    "def check_homework():\n",
    "    train_X, test_X, train_y, test_y = load_dataset()\n",
    "    pred_y = homework(train_X, test_X, train_y)\n",
    "    true_y = []\n",
    "    for instance_y in test_y:\n",
    "        true_y += instance_y\n",
    "    return f1_score(true_y, pred_y, average='macro')\n",
    "\n",
    "if 'homework' in globals():\n",
    "    result = check_homework()\n",
    "    print result\n",
    "    print \"No Error Occured!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ilect": {
     "is_homework": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-853ca6258f22>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-853ca6258f22>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    self.params = [self.V]\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def homework(train_X, test_X, train_y):\n",
    "    # transform to one-hot vector\n",
    "    classes = np.arange(len(tag2index))\n",
    "    train_y = [label_binarize(instance_y, classes).astype('int32') for instance_y in train_y]\n",
    "    # create validation set\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # helper function to create weight/bias\n",
    "    def sharedX(X, name='', dtype=\"float32\"):\n",
    "        return theano.shared(np.array(X, dtype=dtype), name=name)\n",
    "\n",
    "    \n",
    "    class Projection:\n",
    "        \"\"\"Word Embedding Layer\"\"\"\n",
    "        def __init__(self, in_dim, out_dim, scale):\n",
    "            limit = np.sqrt(2./in_dim)\n",
    "            self.V = sharedX(rng.uniform(low=-limit, high=limit, size=(in_dim, out_dim), name='V')\n",
    "            self.params = [self.V]\n",
    "\n",
    "        def f_prop(self, x):\n",
    "            x_emb = self.V[x]\n",
    "            return x_emb\n",
    "        \n",
    "        \n",
    "    class RNN:\n",
    "        \"\"\"Recurrent Layer\"\"\"\n",
    "        def __init__(self, in_dim, hid_dim, scale):\n",
    "            self.hid_dim = hid_dim\n",
    "\n",
    "            # modified initialization\n",
    "            limit_W_in = np.sqrt(2./in_dim)\n",
    "            self.W_in  = sharedX(rng.uniform(low=-limit_W_in, high=limit_W_in, size=(in_dim, hid_dim)), name='W_in')\n",
    "            limit_W_rec = np.sqrt(2./hid_dim)\n",
    "            self.W_rec = sharedX(rng.uniform(low=-limit_W_rec, high=limit_W_rec, size=(hid_dim, hid_dim)), name='W_rec')\n",
    "            # consider how to initialize b\n",
    "            self.b_rec = sharedX(rng.randn(hid_dim) * scale, name='b_rec')\n",
    "            self.h_0   = sharedX(np.zeros(hid_dim), name='h_0')\n",
    "\n",
    "            self.output_info = [self.h_0]\n",
    "            self.params = [self.W_in, self.W_rec, self.b_rec]\n",
    "\n",
    "        def f_prop(self, x):\n",
    "            def step(x, h_tm1):\n",
    "                h = T.dot(x, self.W_in) + T.dot(h_tm1, self.W_rec) + self.b_rec\n",
    "                return h\n",
    "\n",
    "            # loop in theano\n",
    "            h, _ = theano.scan(fn=step,\n",
    "                             sequences=[x],\n",
    "                             outputs_info=self.output_info)\n",
    "            return h\n",
    "        \n",
    "        \n",
    "    class Linear:\n",
    "        \"\"\"Fully-connected Layer\"\"\"\n",
    "        def __init__(self, in_dim, out_dim, scale):\n",
    "            limit = np.sqrt(2./in_dim)\n",
    "            self.W_out = sharedX(rng.uniform(low=-limit, high=limit, size=(in_dim, out_dim), name='W_out')\n",
    "            self.b_out = sharedX(rng.randn(out_dim), name='b_out')\n",
    "            self.params = [self.W_out, self.b_out]\n",
    "\n",
    "        def f_prop(self, x):\n",
    "            z = T.dot(x, self.W_out) + self.b_out\n",
    "            return z\n",
    "        \n",
    "        \n",
    "    class Activation:\n",
    "        \"\"\"Apply activation function to previous layers' outputs\"\"\"\n",
    "        def __init__(self, function):\n",
    "            self.function = function\n",
    "            self.params = []\n",
    "\n",
    "        def f_prop(self, x):\n",
    "            self.z = self.function(x)\n",
    "            return self.z\n",
    "        \n",
    "        \n",
    "    def Adam(params, g_params, lr=0.001, b1=0.1, b2=0.001, e=1e-8):\n",
    "        \"\"\"Adam optimizer with gradient clipping\"\"\"\n",
    "        updates = []\n",
    "        i = theano.shared(np.float32(0.))\n",
    "        i_t = i + 1.\n",
    "        fix1 = 1. - (1. - b1)**i_t\n",
    "        fix2 = 1. - (1. - b2)**i_t\n",
    "        lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "        for p, g in zip(params, g_params):\n",
    "            # consider where to clip\n",
    "            #gnorm = g.norm(L=2)\n",
    "            #rescale = T.maximum(5, gnorm)\n",
    "            #g = g * 5 / rescale\n",
    "            g = T.clip(g, -1, 1)\n",
    "            m = theano.shared(p.get_value() * 0.)\n",
    "            v = theano.shared(p.get_value() * 0.)\n",
    "            m_t = (b1 * g) + ((1. - b1) * m)\n",
    "            v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "            g_t = m_t / (T.sqrt(v_t) + e)\n",
    "            p_t = p - (lr_t * g_t)\n",
    "            updates.append((m, m_t))\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((p, p_t))\n",
    "        updates.append((i, i_t))\n",
    "        return updates\n",
    "    \n",
    "    \n",
    "    # define the structure of networks\n",
    "    vocab_size = len(word2index)\n",
    "    hid_dim    = 400\n",
    "    out_dim    = len(tag2index)\n",
    "\n",
    "    x = T.ivector('x')\n",
    "    t = T.imatrix('t')\n",
    "\n",
    "    layers = [\n",
    "        Projection(vocab_size, 500, scale=0.01),\n",
    "        RNN(500, hid_dim, 0.01),\n",
    "        Activation(T.tanh),\n",
    "        Linear(hid_dim, out_dim, 0.01),\n",
    "        Activation(T.nnet.softmax)\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    # forward pass\n",
    "    params = []\n",
    "    layer_out = x\n",
    "    for i, layer in enumerate(layers):\n",
    "        params += layer.params\n",
    "        if i == 0:\n",
    "            layer_out = layer.f_prop(x)\n",
    "        else:\n",
    "            layer_out = layer.f_prop(layer_out)\n",
    "\n",
    "    y = layers[-1].z\n",
    "    cost = T.mean(T.nnet.categorical_crossentropy(y, t))\n",
    "    # backward pass\n",
    "    gparams = T.grad(cost, params)\n",
    "\n",
    "    # Define update graph\n",
    "    updates = Adam(params, gparams, lr=1e-3, e=1e-3) \n",
    "\n",
    "    # Compile Function\n",
    "    train = theano.function(inputs=[x,t], outputs=cost, updates=updates)\n",
    "    valid = theano.function(inputs=[x,t], outputs=[cost, T.argmax(y, axis=1)])\n",
    "    test  = theano.function(inputs=[x], outputs=T.argmax(y, axis=1))\n",
    "    \n",
    "    \n",
    "    # training\n",
    "    max_epoch = 9\n",
    "    import time\n",
    "    for epoch in xrange(max_epoch):\n",
    "        train_X, train_y = shuffle(train_X, train_y)  # Shuffle Samples !!\n",
    "        start = time.clock()\n",
    "        for i, (instance_x, instance_y) in enumerate(zip(train_X, train_y)):\n",
    "            cost = train(instance_x, instance_y)\n",
    "            if i % 1000 == 0:\n",
    "                print \"EPOCH:: %i, Iteration %i, cost: %.3f\" % (epoch + 1, i, cost)\n",
    "                print \"used time: %.3f\" % (time.clock() - start)\n",
    "                start = time.clock()\n",
    "\n",
    "        true_y, pred_y, valid_cost = [], [], 0\n",
    "        for instance_x, instance_y in zip(valid_X, valid_y):\n",
    "            cost, pred = valid(instance_x, instance_y)\n",
    "            true_y += list(np.argmax(instance_y, axis=1))\n",
    "            pred_y += list(pred)\n",
    "            valid_cost += cost\n",
    "        print 'EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f' % (epoch + 1, valid_cost/len(valid_X), \n",
    "                                                                          f1_score(true_y, pred_y, average='macro'))\n",
    "    \n",
    "    # testing\n",
    "    pred_y = []\n",
    "    for instance_x in test_X:\n",
    "        pred_y += list(test(instance_x))\n",
    "\n",
    "        \n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "ilect": {
     "course_id": 1,
     "course_rank": 10,
     "is_evaluation": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "trng = RandomStreams(42)\n",
    "\n",
    "def load_data(file_path):\n",
    "    dataset = []\n",
    "    vocab, tag = set(), set()\n",
    "    for line in open(file_path):\n",
    "        instance = [l.strip().split() for l in line.split('|||')]\n",
    "        vocab.update(instance[0])\n",
    "        tag.update(instance[1])\n",
    "        dataset.append(instance)\n",
    "    return dataset, vocab, tag\n",
    "\n",
    "def encode_dataset(dataset, word2index, tag2index):\n",
    "    X, y = [], []\n",
    "    vocab = set(word2index.keys())\n",
    "    for sentence, tags in dataset:\n",
    "        X.append([word2index[word] if word in vocab else word2index['<unk>'] for word in sentence])\n",
    "        y.append([tag2index[tag] for tag in tags])\n",
    "    return X, y\n",
    "\n",
    "def load_dataset():\n",
    "    train_data, train_vocab, train_tags = load_data('train.unk')\n",
    "    special_words = set(['<unk>'])\n",
    "    \n",
    "    global word2index\n",
    "    global tag2index\n",
    "\n",
    "    word2index = dict(map(lambda x: (x[1], x[0]), enumerate(train_vocab | special_words)))\n",
    "    tag2index  = dict(map(lambda x: (x[1], x[0]), enumerate(train_tags)))\n",
    "\n",
    "    train_X, train_y = encode_dataset(train_data, word2index, tag2index)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return (train_X, test_X, train_y, test_y)\n",
    "\n",
    "def check_homework():\n",
    "    train_X, test_X, train_y, test_y = load_dataset()\n",
    "    pred_y = homework(train_X, test_X, train_y)\n",
    "    true_y = []\n",
    "    for instance_y in test_y:\n",
    "        true_y += instance_y\n",
    "    return f1_score(true_y, pred_y, average='macro')\n",
    "\n",
    "if 'homework' in globals():\n",
    "    result = check_homework()\n",
    "\n",
    "    print \"No Error Occured!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
